{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do's:\n",
    "- convert it to csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Upload and read documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installations:\n",
    "pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "from docx import Document\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#This is if the load in is a docx, I could not figure out how to deal with the paragraph thing\\n\\n# Specify the path to your .docx file\\n#\"C:\\\\Users\\\\anett\\\\NewFacultyOrientationResearchFAQs.docx\"\\n#\"C:\\\\Users\\\\anett\\\\CopyofNewFacultyOrientationResearch.docx\"\\ndocx_file_path = \\'C:\\\\Users\\\\anett\\\\CopyofNewFacultyOrientationResearch.docx\\'\\n\\n# Open the .docx file\\ndoc = Document(docx_file_path)\\n\\n# Iterate through paragraphs and print the text\\nfor paragraph in doc.paragraphs:\\n        print(paragraph.text)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#This is if the load in is a docx, I could not figure out how to deal with the paragraph thing\n",
    "\n",
    "# Specify the path to your .docx file\n",
    "#\"C:\\\\Users\\\\anett\\\\NewFacultyOrientationResearchFAQs.docx\"\n",
    "#\"C:\\\\Users\\\\anett\\\\CopyofNewFacultyOrientationResearch.docx\"\n",
    "docx_file_path = 'C:\\\\Users\\\\anett\\\\CopyofNewFacultyOrientationResearch.docx'\n",
    "\n",
    "# Open the .docx file\n",
    "doc = Document(docx_file_path)\n",
    "\n",
    "# Iterate through paragraphs and print the text\n",
    "for paragraph in doc.paragraphs:\n",
    "        print(paragraph.text)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extracted from C:\\Users\\anett\\CopyofNewFacultyOrientationResearch.docx and saved to EditedNewFacultyOrientationResearch.txt\n"
     ]
    }
   ],
   "source": [
    "#We are gonna try to convert a docx file to a .txt for easier coding\n",
    "\n",
    "docx_file_path = 'C:\\\\Users\\\\anett\\\\CopyofNewFacultyOrientationResearch.docx'\n",
    "doc = Document(docx_file_path)\n",
    "\n",
    "# Create or open a text file to write the extracted text\n",
    "txt_file_path = 'EditedNewFacultyOrientationResearch.txt'\n",
    "with open(txt_file_path, 'w', encoding='utf-8') as txt_file:\n",
    "    # Iterate through paragraphs and write the text to the text file\n",
    "    for paragraph in doc.paragraphs:\n",
    "        txt_file.write(paragraph.text + '\\n')\n",
    "\n",
    "print(f'Text extracted from {docx_file_path} and saved to {txt_file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I would like to submit a proposal, how do I ge...</td>\n",
       "      <td>Please start a routing sheet (SP proposal) in ...</td>\n",
       "      <td>proposal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How soon should I start my Cayuse routing sheet?</td>\n",
       "      <td>Please start the first page, General tab, of t...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are IIT’s internal deadlines?</td>\n",
       "      <td>The administrative portions of the proposal (b...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How will I know who my assigned research admin...</td>\n",
       "      <td>After you start your routing sheet, OSRP will ...</td>\n",
       "      <td>research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OSRP only handles federally sponsored projects...</td>\n",
       "      <td>No. OSRP handles all sponsored projects for th...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  I would like to submit a proposal, how do I ge...   \n",
       "1   How soon should I start my Cayuse routing sheet?   \n",
       "2                 What are IIT’s internal deadlines?   \n",
       "3  How will I know who my assigned research admin...   \n",
       "4  OSRP only handles federally sponsored projects...   \n",
       "\n",
       "                                              Answer       Tag  \n",
       "0  Please start a routing sheet (SP proposal) in ...  proposal  \n",
       "1  Please start the first page, General tab, of t...     other  \n",
       "2  The administrative portions of the proposal (b...     other  \n",
       "3  After you start your routing sheet, OSRP will ...  research  \n",
       "4  No. OSRP handles all sponsored projects for th...     other  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "So for this I made an edited doc. This is since we can request the \n",
    "ways docs are formatted\n",
    "\n",
    "The format is:\n",
    "Question\n",
    "Answer\n",
    "Tag --> if dont know just put unknown\n",
    "<Space>\n",
    "\n",
    "currently allowing for them to do this in word doc and save as docx since\n",
    " we are allowing for google API I thought it was fitting.\n",
    "This also allows for the CS team to connect with the DB team and be able\n",
    "to edit and delete one and a time. We will connect them with this code if they \n",
    "ever choose to upload a file and this will be run. \n",
    "'''\n",
    "\n",
    "txt_file_path = 'C:\\\\Users\\\\anett\\\\EditedNewFacultyOrientationResearch.txt'\n",
    "questions = []\n",
    "answers = []\n",
    "tags = []\n",
    "\n",
    "# Read the text file and iterate through non-empty lines\n",
    "with open(txt_file_path, 'r', encoding='utf-8') as txt_file:\n",
    "    lines = [line.strip() for line in txt_file.readlines() if line.strip()]\n",
    "    \n",
    "    # Iterate through lines in sets of 3\n",
    "    for i in range(0, len(lines), 3):\n",
    "        # Check if there are enough lines left to extract a set\n",
    "        if i + 2 < len(lines):\n",
    "            questions.append(lines[i])\n",
    "            answers.append(lines[i + 1])\n",
    "            tags.append(lines[i + 2])\n",
    "        else:\n",
    "            # Handle the case where there are not enough lines for a complete set\n",
    "            print(f\"Skipping incomplete set at index {i}.\")\n",
    "\n",
    "# Create a DataFrame from the lists of questions, answers, and tags\n",
    "df = pd.DataFrame({'Question': questions, 'Answer': answers, 'Tag': tags})\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the dataframe into mySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installations which may be necessary\n",
    "#pip install pandas sqlalchemy pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "#from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Replace these variables with your MySQL database credentials\n",
    "db_username = 'your_username'\n",
    "db_password = 'your_password'\n",
    "db_host = 'your_host'\n",
    "db_port = 'your_port'\n",
    "db_name = 'your_database_name'\n",
    "\n",
    "# Create a connection string for SQLAlchemy\n",
    "connection_string = f'mysql+pymysql://{db_username}:{db_password}@{db_host}:{db_port}/{db_name}'\n",
    "\n",
    "# Create a SQLAlchemy engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Replace df with your DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Question': ['Q1', 'Q2', 'Q3'],\n",
    "    'Answer': ['A1', 'A2', 'A3'],\n",
    "    'Tag': ['T1', 'T2', 'T3']\n",
    "})\n",
    "\n",
    "# Replace 'your_table_name' with the desired table name\n",
    "table_name = 'your_table_name'\n",
    "\n",
    "# Write the DataFrame to MySQL\n",
    "df.to_sql(name=table_name, con=engine, if_exists='replace', index=False)\n",
    "\n",
    "print(f'DataFrame successfully written to MySQL table: {table_name}')\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Grab user question and give it filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, nice to meet you I am the fake chatbot.\n",
      "Please select the group your question fits in: \n",
      "proposal, research, policy, other\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print('Hello, nice to meet you I am the fake chatbot.')\n",
    "print('Please select the group your question fits in: ')\n",
    "print('proposal, research, policy, other')\n",
    "print(' ')\n",
    "user_input = \"proposal\"\n",
    "\n",
    "print('Here are some of our top asked questions for proposals.')\n",
    "print('Please ask your question if it doesnt fit those.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step: embedd as vectors the QA's and tags\n",
    "My recommednation is to use distill bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So my thought here is we can make the identical database as the other one just in vector representation. We only vector represent questions and tags since its all connected we can access both databases and get the answer based on the vectorized question... \n",
    "I am not sure how the updating would work tho. Would the vectorized database update each time a new entry gets placed in the new database???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installations that may be necessary\n",
    "#pip install transformers\n",
    "#pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import sqlite3\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to convert text to BERT embeddings\n",
    "def get_bert_embedding(text):\n",
    "    tokens = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "    # Use the [CLS] token embedding as the representation of the entire sequence\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Connect to database\n",
    "conn = sqlite3.connect('your_database.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Get QA entries from the database\n",
    "cursor.execute(\"SELECT question, answer, tag FROM qa_table\")\n",
    "qa_entries = cursor.fetchall()\n",
    "\n",
    "# Convert QA entries to vectors using BERT embeddings\n",
    "vectors = []\n",
    "for question, answer in qa_entries:\n",
    "    question_vector = get_bert_embedding(question)\n",
    "    answer_vector = get_bert_embedding(answer)\n",
    "    tag_vector = get_bert_embedding(tag)\n",
    "    vectors.append((question_vector, answer_vector, tag_vector))\n",
    "\n",
    "# Now 'vectors' contains the BERT embeddings for each QA pair\n",
    "# Now we can use for comparison and similarity scores\n",
    "\n",
    "# Close connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Move the vectors to a database\n",
    "This is so we do not have to reconvert them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installations which may be necessary\n",
    "#pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "#import torch\n",
    "#from transformers import BertTokenizer, BertModel\n",
    "#import mysql.connector\n",
    "#import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to move them to a database\n",
    "'''\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to convert text to BERT embeddings\n",
    "def get_bert_embedding(text):\n",
    "    tokens = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "    # Use the [CLS] token embedding as the representation of the entire sequence\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Connect to your source database (MySQL)\n",
    "source_conn = mysql.connector.connect(\n",
    "    host='your_source_host',\n",
    "    user='your_username',\n",
    "    password='your_password',\n",
    "    database='your_source_database'\n",
    ")\n",
    "source_cursor = source_conn.cursor()\n",
    "\n",
    "# Fetch QA entries from the source database\n",
    "source_cursor.execute(\"SELECT question, answer FROM qa_table\")\n",
    "qa_entries = source_cursor.fetchall()\n",
    "\n",
    "# Convert QA entries to vectors using BERT embeddings\n",
    "vectors = []\n",
    "for question, answer in qa_entries:\n",
    "    question_vector = get_bert_embedding(question)\n",
    "    answer_vector = get_bert_embedding(answer)\n",
    "    vectors.append((question_vector, answer_vector))\n",
    "\n",
    "# Close the source database connection\n",
    "source_conn.close()\n",
    "\n",
    "# Connect to your destination database (MySQL)\n",
    "dest_conn = mysql.connector.connect(\n",
    "    host='your_dest_host',\n",
    "    user='your_username',\n",
    "    password='your_password',\n",
    "    database='your_dest_database'\n",
    ")\n",
    "dest_cursor = dest_conn.cursor()\n",
    "\n",
    "# Create a table to store vectors in the destination database\n",
    "dest_cursor.execute('''\n",
    "    #CREATE TABLE IF NOT EXISTS vector_table (\n",
    "    #    question_vector BLOB,\n",
    "    #    answer_vector BLOB\n",
    "    #)\n",
    "''')\n",
    "\n",
    "# Insert vectors into the destination database\n",
    "for question_vector, answer_vector in vectors:\n",
    "    dest_cursor.execute(\"INSERT INTO vector_table (question_vector, answer_vector) VALUES (%s, %s)\",\n",
    "                        (question_vector.tobytes(), answer_vector.tobytes()))\n",
    "\n",
    "# Commit changes and close the destination database connection\n",
    "dest_conn.commit()\n",
    "dest_conn.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: User entry"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
